\documentclass[letterpaper,11pt,twoside]{article}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage[margin=1.0in]{geometry}
\usepackage{fancyhdr, lastpage}
\usepackage[pdftex]{graphicx}
\pdfinfo{
   /Author (Travis Hall, Brittany Miller and Bhadresh Patel) 
   /Title (Creepy - Project Report)
}

\setlength{\parskip}{0.5ex}
\pagestyle{fancy}
\setlength{\headheight}{14.0pt}
\fancyhead{}
\fancyfoot{}
\fancyhead[RO,RE] {Project Report: \emph{Creepy - Web Crawler}}
\fancyfoot[CO,CE] {CS 453: Project 1 - Crawling the Web}
\fancyfoot[RO,RE] {Page \thepage\ of \pageref{LastPage}}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0.5pt}

\begin{document}

%%%%%%%%%% Title Page %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlepage}
   \begin{center}
       {\Large \textbf{CS 453: Project 1 - Crawling the Web}}\\[0.5cm]
       {\Large \textbf{Project Report}}\\[3.0cm]

       {\rule{\linewidth}{0.5mm}} \\[0.5cm]
       {\Huge \textbf{Creepy - Web Crawler}}\\[0.4cm] 
       {\rule{\linewidth}{0.5mm}} \\[2.0cm]

       \textbf{Travis Hall}\\
       \texttt{trvs.hll@gmail.com}\\[0.5cm]
       \textbf{Brittany Miller}\\
       \texttt{miller317@gmail.com}\\[0.5cm]
       \textbf{Bhadresh Patel}\\
       \texttt{bhadresh@wsu.edu}\\[0.5cm]

       \vfill
       Washington State University Vancouver\\
       September 12, 2010
   \end{center}
\end{titlepage}

\begin{abstract}
The main goal of this project is to design and implement a \emph{web crawler}. The name of our web crawler is \emph{Creepy}. \emph{Creepy} is simple web crawler written in Python that takes a set of seeds (URLs) and begins crawling the web.
\end{abstract}

\section{Overview}

\section{Automation}

\section{Document Storage}

\section{Politeness}
When implementing the politeness standards, one of the things we really wanted to do was create a nice, reusable and generic library for the politeness standards. To this end, the module is designed such that you simply create a \emph{Robot} object for a domain and then query whether or not you are allowed to access a URI. In order to ensure that \emph{robots.txt} does not go stale, the programmer can pass along an `expires\_in' value. When the \emph{Robot} is queried, it will then check whether the file is expired and automatically re-fetch and parse it when that is the case.

However, we also needed to design towards the state of the project as a whole, and one of the concerns was how to handle delays. In order to avoid duplicating a domain-based hash for each delay, reusing our \emph{RobotStorage} class (and thus our \emph{Robots}) seemed a natural choice. Unfortunately the result feels a little awkward, in that you have to update the \emph{Robot} and inform it when the last request was made. By preference, this is something that would occur naturally while fetching the page.

Sadly, the \emph{Robot} does not obey the extended standards for \emph{robots.txt}. Though it certainly will parse out that information, it will be stored in the `other' field and is unused unless specifically programmed for. This means that information like `Visit-time', `Request-rate', and `Comment' are largely left ignored. This is not to say that it cannot be extended to obey them, but we had not discovered the extended standard until after the parser was already written and in use.

\section{Duplicate Detection}

\section{Roles}

\begin{description}
 \item[Travis Hall] Politeness standards, project management (Git/Github)
 \item[Brittany Miller] ... 
 \item[Bhadresh Patel] ...
\end{description}

\end{document}