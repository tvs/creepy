\documentclass[letterpaper,11pt,twoside]{article}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage[margin=1.0in]{geometry}
\usepackage{fancyhdr, lastpage}
\usepackage[pdftex]{graphicx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor={black},
    linkcolor={black},
    urlcolor={black},
    bookmarksnumbered,
    pdfstartview={FitH},
    pdfpagemode={UseOutlines},
    pdftitle={Creepy - Project Report},
    pdfauthor={Travis Hall, Brittany Thompson and Bhadresh Patel},
    pdfsubject={CS 453 Project}
}

\setlength{\parskip}{0.5ex}
\pagestyle{fancy}
\setlength{\headheight}{14.0pt}
\fancyhead{}
\fancyfoot{}
\fancyhead[RO,RE] {Project Report: \emph{Creepy - Data Cleanser}}
\fancyfoot[LO,LE] {CS 453: Project 3 - PageRank and Indexing}
\fancyfoot[RO,RE] {Page \thepage\ of \pageref{LastPage}}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0.5pt}

\begin{document}

%%%%%%%%%% Title Page %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlepage}
   \begin{center}
       {\Large \textbf{CS 453: Project 3 - PageRank and Indexing}}\\[0.5cm]
       {\Large \textbf{Project Report}}\\[3.0cm]

       {\rule{\linewidth}{0.5mm}} \\[0.5cm]
       {\Huge \textbf{Creepy - Indexer}}\\[0.4cm] 
       {\rule{\linewidth}{0.5mm}} \\[2.0cm]

       \textbf{Travis Hall}\\
       \texttt{trvs.hll@gmail.com}\\[0.5cm]
       \textbf{Brittany Thompson}\\
       \texttt{miller317@gmail.com}\\[0.5cm]
       \textbf{Bhadresh Patel}\\
       \texttt{bhadresh@wsu.edu}\\[0.5cm]

       \vfill
       Washington State University Vancouver\\
       October 31, 2010
   \end{center}
\end{titlepage}

\begin{abstract}
The main goal of this project is to index terms in all of the documents, rank them, and get ready for keyword queries. Both the page ranking and the indexing is done using the Map-Reduce paradigm.
\end{abstract}

\section{Overview}
For project 2, we used the Map-Reduce paradigm developed by Google for web crawlers for page ranking and term indexing. We take the documents that have already been tokenized, stopped, and stemmed in order to make a list of all the imperative terms, as well as determine the pageâ€™s importance using a Google-like page ranking algorithm.

\section{PageRank}

\section{Indexing}
After the stopping and stemming is complete, the indexer is then ran to catalog all the terms in every document. The indexer is written in python and uses an inverted list structure similar to the indices in the back of textbooks. I used a nested dictionary to hold the list of terms and with each term is attached a list of pages and the occurrences of that term.  On the test documents that I used, the output looks something like this:
{..., 'international': {'3testPage.txt': 1, '4testPage.txt': 1, '5testPage.txt': 1}, 'security': {'3testPage.txt': 11, '4testPage.txt': 2}, ...}
where the term is listed along with the page that it is found on and the number of times it appears on that page.

\section{MapReduce Indexing}
While the previous indexer certainly works when run on a single machine, it runs into trouble when being distributed. Since our document corpus can become quite large, we clearly needed a way to distribute the creation of our indices: this is where MapReduce comes in.

One of the first things that was required for doing MapReduce was some extra processing done to the files. Since the way MapReduce works is to simply stream the file into the mapper's STDIN, we needed to find a way to tag the file with its ID. Fortunately, since all the files had, at this point, been processed and stopped and stemmed, we knew that there would not be any HTML (or XML)-like tags still within the document. The obvious answer to tagging was then to simply introduce a new tag as the first line of the document: \texttt{$<$file=[filename]$>$}. We also needed to ensure that there's a newline character at the end of each file so that when processing we only had to run a regular expression on each line instead of each word individually. All this is done in \texttt{prep\_data.rb}. Obviously, doing this required a bit of extra processing time, so ideally this would be implemented into Creepy-proper and the tags written as everything else is.

Then the mapper (\texttt{lib/mapreduce/map.rb}) and Reducer (\texttt{lib/mapreduce/reduce.rb}) are fairly straight forward. Whenever the mapper sees a line containing the tag, it considers all input between that line and the next occurrence of the tag as belonging to the indicated document. From there, it simply breaks up the lines into the individual words and tallies up a count. Once completed, it outputs the various words with its list of occurrences (and their various document-based tallies) that will get sorted and fed to the reducer.

The reducer simply takes the input from the mapper and creates another hash, mapping words to $(document:occurrence)$ pairs. It also maintains a cumulative tally this time. Finally, it sorts the hash alphabetically by key (word) and outputs in a format identical to the assignment document.

Running this through MapReduce was done in the same way as we did for homework three. In this case, the processed copies of the documents were uploaded to S3 and then the whole process was started via SSH into WSUV's CS server. Once completed. The resulting partial-indices can then be merged, split, or updated as needed in the future.

\section{Roles}
\begin{description}
 \item[Travis Hall] MapReduce indexing,
 \item[Brittany Thompson] Indexing
 \item[Bhadresh Patel] Basic PageRank, MapReduce PageRank, script to run MapReduce PageRank iteratively until convergence.
\end{description}

\section{Test Environment}
For testing/production purpose, we set up a machine instance on Amazon EC2. The instance id of the machine is \texttt{i-7d1e0d17}. The source code is checked out at \texttt{/home/ubuntu/creepy/}.

\section{Usage Guide}

\subsection{PageRank}
\begin{verbatim}
  $ python lib/pagerank/PageRank -l linkmap.xml
\end{verbatim}
	
\subsection{Indexing}

\end{document}
