\documentclass[letterpaper,11pt,twoside]{article}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage[margin=1.0in]{geometry}
\usepackage{fancyhdr, lastpage}
\usepackage[pdftex]{graphicx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor={black},
    linkcolor={black},
    urlcolor={black},
    bookmarksnumbered,
    pdfstartview={FitH},
    pdfpagemode={UseOutlines},
    pdftitle={Creepy - Project Report},
    pdfauthor={Travis Hall, Brittany Thompson and Bhadresh Patel},
    pdfsubject={CS 453 Project}
}

\setlength{\parskip}{0.5ex}
\pagestyle{fancy}
\setlength{\headheight}{14.0pt}
\fancyhead{}
\fancyfoot{}
\fancyhead[RO,RE] {Project Report: \emph{Creepy - PageRank and Indexing}}
\fancyfoot[LO,LE] {CS 453: Project 3 - PageRank and Indexing}
\fancyfoot[RO,RE] {Page \thepage\ of \pageref{LastPage}}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0.5pt}

\begin{document}

%%%%%%%%%% Title Page %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlepage}
   \begin{center}
       {\Large \textbf{CS 453: Project 3 - PageRank and Indexing}}\\[0.5cm]
       {\Large \textbf{Project Report}}\\[3.0cm]

       {\rule{\linewidth}{0.5mm}} \\[0.5cm]
       {\Huge \textbf{Creepy - Indexer}}\\[0.4cm] 
       {\rule{\linewidth}{0.5mm}} \\[2.0cm]

       \textbf{Travis Hall}\\
       \texttt{trvs.hll@gmail.com}\\[0.5cm]
       \textbf{Brittany Thompson}\\
       \texttt{miller317@gmail.com}\\[0.5cm]
       \textbf{Bhadresh Patel}\\
       \texttt{bhadresh@wsu.edu}\\[0.5cm]

       \vfill
       Washington State University Vancouver\\
       October 31, 2010
   \end{center}
\end{titlepage}

\begin{abstract}
The main goal of this project is to index terms in all of the documents, rank them, and get ready for keyword queries. Both the page ranking and the indexing is done using the MapReduce paradigm.
\end{abstract}

\section{Overview}
In this project, we used the MapReduce paradigm developed by Google, for page ranking and term indexing. We take the documents that have already been tokenized, stopped, and stemmed in order to make a list of all the imperative terms, as well as determine the pageâ€™s importance using a Google-like page ranking algorithm called \emph{PageRank}.

\section{PageRank}
There are tens of billions of web pages, which makes a difficult problem for a search engine to rank web pages effectively. One very effective approach is to use the links between web pages as a way to measure popularity and rank web pages using its popularity. Algorithms that calculates such measures based on link structure between pages are often referred to as the \emph{link analysis algorithm}. \emph{PageRank}, is one of the most popular link analysis algorithm used by Google search engine. We will use PageRank to compute importance of a page for our search engine. PageRank is based on the idea of a \emph{random surfer}, where the random surfer visits a web page with a certain probability which derives from the page's PageRank. The probability that the random surfer clicks on one link is solely given by the number of links on that page. The PageRank of a page $u$ is defined as follows,

\begin{equation}
	PR(u) = \frac{\lambda}{N} + (1 - \lambda) \cdot \sum_{v \in B_u} \frac{PR(v)}{L_v}
\end{equation}
where $N$ is the number of pages and typical value for $\lambda$ is 0.15. Lawrence Page and Sergey Brin have published two different versions of their PageRank algorithm in different papers. In second paper\footnote{http://ilpubs.stanford.edu:8090/361/} they represented PageRank of a page $u$ as follows,
\begin{equation}
	PR(u) =  (1 - d) + d \cdot \sum_{v \in B_u} \frac{PR(v)}{L_v}
\end{equation}  
where $d$ is a damping factor and its typical value is 0.85. In both equations, $B_u$ is the set of pages that point to $u$, and $L_v$ is the number of unique outgoing links from page $v$.

In the first equation, PageRank of a page is the actual probability for a surfer reaching that page after clicking on many links and the sum of all pages' PageRanks will be one. Contrary, in the second equation, the probability for the random surfer reaching a page is weighted by the total number of web pages. The both version of algorithm do not differ fundamentally from each other. Hence, we will use the later version of the equation to calculate PageRank of a page in our project. The reason is that in this version calculation of PageRank are easier because we don't need value of the total number of web pages. 

\subsection{Basic Implementation}
First, we implemented calculation of PageRank sequentially using the link analysis graph we generated in the previous project. The link analysis graph contains lists all documents, their inlinks, and outlinks. The implementation simply consumes the xml document generated by link analysis module and calculate PageRank of each document. The process is repeated until all values \emph{converge}; \emph{i.e}, does not change significantly between iterations.  

\subsection{MapReduce Implementation}
Implementing calculation of PageRank in MapReduce paradigm was a rather challenging task. The implementation is done in following three stages:
\subsubsection{Data Preparation}
In the first stage, we process the link analysis graph and generate necessary input data for the MapReduce program. We generate multiple input files, each containing 50 documents. The format of the input files is:
\begin{verbatim}
Docid	Initial_PageRank	Outlinks
\end{verbatim}

Where, \texttt{Docid} is the id of the web page, \texttt{Initial\_PageRank} is the initial PageRank for the web page, and \texttt{Outlinks} is the list of all outgoing links from the web page. For example,
\begin{verbatim}
1	1.0	2,3
2	1.0	3
3	1.0	1
\end{verbatim}

\subsubsection{Iterative MapReduce}
The second stage consumes the input data and iteratively calculates PageRank using MapReduce paradigm until convergence. The \emph{mapper} task takes input as described in the previous section. For each page, it outputs the \texttt{Docid} and list of \texttt{Outlinks}. For each outlinks, it also outputs \texttt{Docid} of the \texttt{Outlinks} and its PageRank. For example,
\begin{verbatim}
1	[2,3]
2	0.5
3	0.5
2	[3]
3	1.0
3	[1]
1	1.0
\end{verbatim}
The \emph{reducer} task consumes the output of mapper task and calculates actual PageRank of each pages. Next, it generates the output in the same way as the input to the mapper task. Thus, at the end of the first iteration example output from reducer task is:
\begin{verbatim}
1	1.0	2,3
2	0.575	3
3	1.425	1
\end{verbatim}

We need to repeat this process until all the PageRank values are stabilize (does not change significantly). To run the above MapReduce implementation iteratively until PageRanks are converged, we wrote a driver script that starts a streaming job flow for Amazon Elastic MapReduce. The script utilizes a Python library called \emph{boto}\footnote{http://code.google.com/p/boto/}, which provides Python interface to Amazon Web Services. The script also uses another tool called \emph{S3cmd}\footnote{http://s3tools.org/s3cmd}, for communicating with Amazon S3. Upon execution, the script first uploads the PageRank calculation code and input files to Amazon S3. Next, it creates a new job flow and add \texttt{s}, typically 10, steps to it. After each \texttt{s} steps, downloads the last two iteration output and check for convergence. If PageRanks are not converged, will add another \texttt{s} steps. This process is repeated until convergence, at the end, script executes the final step as described next, downloads the final output, and terminate the job flow.  

\subsubsection{Final MapReduce Step}
Once the PageRank calculation has been converged, one additional MapReduce step is executed to generate necessary output as required by the project. In this step, the mapper task simple prints \texttt{Docid} and \texttt{PageRank}, whereas the reducer task outputs as follows,
\begin{verbatim}
1:1.163369
2:0.644432
3:1.192199
\end{verbatim}

\section{Indexing}
After the stopping and stemming is complete, the indexer is then ran to catalog all the terms in every document. The indexer is written in python and uses an inverted list structure similar to the indices in the back of textbooks. I used a nested dictionary to hold the list of terms and with each term is attached a list of pages and the occurrences of that term.  On the test documents that I used, the output looks something like this:
{..., `international': {`3testPage.txt': 1, `4testPage.txt': 1, `5testPage.txt': 1}, `security': {`3testPage.txt': 11, `4testPage.txt': 2}, ...}
where the term is listed along with the page that it is found on and the number of times it appears on that page.  

\section{MapReduce Indexing}
While the previous indexer certainly works when run on a single machine, it runs into trouble when being distributed. Since our document corpus can become quite large, we clearly needed a way to distribute the creation of our indices: this is where MapReduce comes in.

One of the first things that was required for doing MapReduce was some extra processing done to the files. Since the way MapReduce works is to simply stream the file into the mapper's STDIN, we needed to find a way to tag the file with its ID. Fortunately, since all the files had, at this point, been processed and stopped and stemmed, we knew that there would not be any HTML (or XML)-like tags still within the document. The obvious answer to tagging was then to simply introduce a new tag as the first line of the document: \texttt{$<$file=[filename]$>$}. We also needed to ensure that there's a newline character at the end of each file so that when processing we only had to run a regular expression on each line instead of each word individually. All this is done in \texttt{prep\_data.rb}. Obviously, doing this required a bit of extra processing time, so ideally this would be implemented into Creepy-proper and the tags written as everything else is.

Then the mapper (\texttt{lib/mapreduce/map.rb}) and Reducer (\texttt{lib/mapreduce/reduce.rb}) are fairly straight forward. Whenever the mapper sees a line containing the tag, it considers all input between that line and the next occurrence of the tag as belonging to the indicated document. From there, it simply breaks up the lines into the individual words and tallies up a count. Once completed, it outputs the various words with its list of occurrences (and their various document-based tallies) that will get sorted and fed to the reducer.

The reducer simply takes the input from the mapper and creates another hash, mapping words to $(document:occurrence)$ pairs. It also maintains a cumulative tally this time. Finally, it sorts the hash alphabetically by key (word) and outputs in a format identical to the assignment document.

Running this through MapReduce was done in the same way as we did for homework three. In this case, the processed copies of the documents were uploaded to S3 and then the whole process was started via SSH into WSUV's CS server. Once completed. The resulting partial-indices can then be merged, split, or updated as needed in the future.

\section{Roles}
\begin{description}
 \item[Travis Hall] MapReduce indexing,
 \item[Brittany Thompson] Indexing
 \item[Bhadresh Patel] Basic PageRank, MapReduce PageRank, script to run MapReduce PageRank iteratively until convergence.
\end{description}

\section{Test Environment}
For testing/production purpose, we set up a machine instance on Amazon EC2. The instance id of the machine is \texttt{i-7d1e0d17}. The source code is checked out at \texttt{/home/ubuntu/creepy/}.

\section{Usage Guide}

\subsection{PageRank}
\begin{itemize}
	\item To calculate PageRank sequentially using link map
\begin{verbatim}
	$ ./lib/pagerank/PageRank -l linkmap.xml
\end{verbatim}

	\item Prepare data for MapReduce PageRank calculation
\begin{verbatim}
	$ ./lib/pagerank/PageRank.py -p storage/links.xml
\end{verbatim}
	
	\item Run MapReduce PageRank iteratively until convergence (boto and s3cmd is required to run this script)
\begin{verbatim}
	$ ./lib/pagerank/run.py -c 0.0000001 -n 5
\end{verbatim}
\end{itemize}

\subsection{Indexing}
\begin{itemize}
   \item To run the Mapper and Reducer without utilizing the Cloud
   \begin{verbatim}
$ cat [corpus directory]/* | ./map.rb | sort | ./reduce.rb
   \end{verbatim}
   \item Running over the Cloud is handled in the same manner as homework three (using the supplied scripts with the necessary changes).
\end{itemize}

\end{document}
