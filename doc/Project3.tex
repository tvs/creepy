\documentclass[letterpaper,11pt,twoside]{article}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage[margin=1.0in]{geometry}
\usepackage{fancyhdr, lastpage}
\usepackage[pdftex]{graphicx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor={black},
    linkcolor={black},
    urlcolor={black},
    bookmarksnumbered,
    pdfstartview={FitH},
    pdfpagemode={UseOutlines},
    pdftitle={Creepy - Project Report},
    pdfauthor={Travis Hall, Brittany Thompson and Bhadresh Patel},
    pdfsubject={CS 453 Project}
}

\setlength{\parskip}{0.5ex}
\pagestyle{fancy}
\setlength{\headheight}{14.0pt}
\fancyhead{}
\fancyfoot{}
\fancyhead[RO,RE] {Project Report: \emph{Creepy - Data Cleanser}}
\fancyfoot[LO,LE] {CS 453: Project 3 - PageRank and Indexing}
\fancyfoot[RO,RE] {Page \thepage\ of \pageref{LastPage}}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0.5pt}

\begin{document}

%%%%%%%%%% Title Page %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlepage}
   \begin{center}
       {\Large \textbf{CS 453: Project 3 - PageRank and Indexing}}\\[0.5cm]
       {\Large \textbf{Project Report}}\\[3.0cm]

       {\rule{\linewidth}{0.5mm}} \\[0.5cm]
       {\Huge \textbf{Creepy - Indexer}}\\[0.4cm] 
       {\rule{\linewidth}{0.5mm}} \\[2.0cm]

       \textbf{Travis Hall}\\
       \texttt{trvs.hll@gmail.com}\\[0.5cm]
       \textbf{Brittany Thompson}\\
       \texttt{miller317@gmail.com}\\[0.5cm]
       \textbf{Bhadresh Patel}\\
       \texttt{bhadresh@wsu.edu}\\[0.5cm]

       \vfill
       Washington State University Vancouver\\
       October 31, 2010
   \end{center}
\end{titlepage}

\begin{abstract}
The main goal of this project is to index terms in all of the documents, rank them, and get ready for keyword queries. Both the page ranking and the indexing is done using the Map-Reduce paradigm.
\end{abstract}

\section{Overview}
For project 2, we used the Map-Reduce paradigm developed by Google for web crawlers for page ranking and term indexing. We take the documents that have already been tokenized, stopped, and stemmed in order to make a list of all the imperative terms, as well as determine the pageâ€™s importance using a Google-like page ranking algorithm.

\section{PageRank}

\section{Indexing}
After the stopping and stemming is complete, the indexer is then ran to catalog all the terms in every document. The indexer is written in python and uses an inverted list structure similar to the indices in the back of textbooks. I used a nested dictionary to hold the list of terms and with each term is attached a list of pages and the occurrences of that term.  On the test documents that I used, the output looks something like this:
{..., 'international': {'3testPage.txt': 1, '4testPage.txt': 1, '5testPage.txt': 1}, 'security': {'3testPage.txt': 11, '4testPage.txt': 2}, ...}
where the term is listed along with the page that it is found on and the number of times it appears on that page.  

\section{Roles}
\begin{description}
 \item[Travis Hall] 
 \item[Brittany Thompson] Indexing
 \item[Bhadresh Patel] Basic PageRank, MapReduce PageRank, script to run MapReduce PageRank iteratively until convergence.
\end{description}

\section{Test Environment}
For testing/production purpose, we set up a machine instance on Amazon EC2. The instance id of the machine is \texttt{i-7d1e0d17}. The source code is checked out at \texttt{/home/ubuntu/creepy/}.

\section{Usage Guide}

\subsection{PageRank}
\begin{verbatim}
  $ python lib/pagerank/PageRank -l linkmap.xml
\end{verbatim}
	
\subsection{Indexing}

\end{document}
